{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch-MNIST.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukCChkRx0db6",
        "colab_type": "text"
      },
      "source": [
        "Autora: Júlia Pelayo Rodrigues\n",
        "```\n",
        "```\n",
        "GitHub: @jpelax\n",
        "```\n",
        "```\n",
        "Baseado principalmente no [PyTorch 1.2 Quickstart with Google Colab](https://medium.com/dair-ai/pytorch-1-2-quickstart-with-google-colab-6690a30c38d)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IEAqtrQHlHs",
        "colab_type": "code",
        "outputId": "62ffd859-cf17-41d7-b1c1-2800cc1891b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        }
      },
      "source": [
        "#importações necessárias\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "# tamanho do pacote de dados que será carregado para a rede de uma vez\n",
        "BATCH_SIZE = 5\n",
        "\n",
        "# função que serausada para transformar os dados em tensors\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor()])\n",
        "\n",
        "# download do dataset de treino (e transformação em tensors)\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "# estrutura do pytorch que facilita carregar os dados para a rede neural em batches\n",
        "# mais tarde usada como iterador\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "# download, transformação em tensor e criação do dataloader do dataset de teste\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE,\n",
        "                                         shuffle=False, num_workers=2)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "9920512it [00:01, 4998396.70it/s]                             \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/28881 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 122855.62it/s]           \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1654784it [00:00, 1897905.03it/s]                            \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "8192it [00:00, 45874.09it/s]            \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hFpzY7f1Smt",
        "colab_type": "text"
      },
      "source": [
        "#Entendendo o dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaW6LDF1KWwi",
        "colab_type": "code",
        "outputId": "a35a9cb7-d72f-4ce1-c2ef-b4de47a43653",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "import numpy as np \n",
        "\n",
        "#cria iterador do dataset de treino (com o dataloader)\n",
        "train_iter = iter(trainloader)\n",
        "\n",
        "images, labels = train_iter.next()\n",
        "\n",
        "print(type(images)) #tipo Tensor da biblioteca torch\n",
        "print(images.shape) #[tamanho_batch, 1, 28, 28] (28x28, tamanho das imagens do MNIST)\n",
        "\n",
        "print(type(labels)) \n",
        "print(labels.shape) #[tamanho_batch] -> uma label para cada imagem\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'torch.Tensor'>\n",
            "torch.Size([5, 1, 28, 28])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([5])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4D38ufxQB3y",
        "colab_type": "code",
        "outputId": "e8445500-f867-4858-a788-819553c8e815",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "#usando matplotlib para visualizar os dados contidos em um batch\n",
        "\n",
        "npimgs = torchvision.utils.make_grid(images).numpy() #utilidade que transforma o tensor torch em um array numpy\n",
        "plt.imshow(np.transpose(npimgs, (1, 2, 0)))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f0158ee1b00>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAABpCAYAAAAqXNiiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fu2DtEDxfxiD",
        "colab_type": "text"
      },
      "source": [
        "# **Construindo o modelo**\n",
        "Para esse modelo serão usados 3 layers lineares, o primeiro com um input de tamanho 28*28 (784), output 500 e funçaõ de ativação ReLU, o segundo com input de tamanho 500 e saída 256, também usando ReLU, e o terceiro, o layer de output, com 10 labels de saída, usando a função de ativaçã Softmax."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQcGAsFISig3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        #layer 0: 28x28 -> 500\n",
        "        self.d0 = nn.Linear(784, 16)\n",
        "        #layer 1: 500 -> 256\n",
        "        self.d1 = nn.Linear(16, 16)\n",
        "        #layer 2 (output): 256 -> 10\n",
        "        self.d2 = nn.Linear(16, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #funçã que define como será o feedforward, não é chamada diretamente\n",
        "        \n",
        "        x = x.flatten(start_dim = 1) \n",
        "        \n",
        "        #input passa pelo primeiro layer\n",
        "        x = self.d0(x)\n",
        "        x = F.relu(x)  #função de ativação\n",
        "\n",
        "        #input passa pelo segundo layer\n",
        "        x = self.d1(x)\n",
        "        x = F.relu(x)\n",
        "        \n",
        "        #layer de output\n",
        "        logits = self.d2(x)\n",
        "        out = F.softmax(logits, dim=1) #usando a função de ativação softmax\n",
        "        return out\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bAmVtyib9lM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "learning_rate = 0.001\n",
        "num_epochs = 5\n",
        "\n",
        "#define se as operacoes serao executadas pela cpu ou gpu\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n",
        "\n",
        "#instancia o modelo\n",
        "model = Net()\n",
        "model = model.to(device)\n",
        "\n",
        "# função de erro\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# define o otimizador\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# computa a acuracia\n",
        "def get_accuracy(logit, target, batch_size):\n",
        "    ''' Obtain accuracy for training round '''\n",
        "    corrects = (torch.max(logit, 1)[1].view(target.size()).data == target.data).sum()\n",
        "    accuracy = 100.0 * corrects/batch_size\n",
        "    return accuracy.item()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6COaTF5cBSR",
        "colab_type": "code",
        "outputId": "00c61e38-a7ba-4773-89f2-cb6d360efbd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "#treino\n",
        "for epoch in range(num_epochs):\n",
        "    train_running_loss = 0.0\n",
        "    train_acc = 0.0\n",
        "\n",
        "    model = model.train()\n",
        "    #define que a modelo está em modo treinamento\n",
        "\n",
        "    ## training step\n",
        "    for i, (images, labels) in enumerate(trainloader):\n",
        "        #manda ou para a cpu ou gpu\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        #carrega o batch para a rede e recebe o output\n",
        "        logits = model(images)\n",
        "        #computa o erro\n",
        "        loss = criterion(logits, labels)\n",
        "        #realiza o backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # update dos parametros\n",
        "        optimizer.step()\n",
        "\n",
        "        # computa a loss e a acuracia\n",
        "        train_running_loss += loss.detach().item()\n",
        "        train_acc += get_accuracy(logits, labels, BATCH_SIZE)\n",
        "    \n",
        "    #define que o modelo está no modo evaluation\n",
        "    model.eval()\n",
        "    \n",
        "    print('Epoch: %d | Loss: %.4f | Train Accuracy: %.2f' \\\n",
        "          %(epoch, train_running_loss / i, train_acc/i)) \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 | Loss: 1.5973 | Train Accuracy: 87.55\n",
            "Epoch: 1 | Loss: 1.5365 | Train Accuracy: 92.72\n",
            "Epoch: 2 | Loss: 1.5273 | Train Accuracy: 93.54\n",
            "Epoch: 3 | Loss: 1.5222 | Train Accuracy: 94.00\n",
            "Epoch: 4 | Loss: 1.5199 | Train Accuracy: 94.20\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}